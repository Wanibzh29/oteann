{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rbM2tC3zmPAn",
    "outputId": "18ecf2b8-2e4f-40ad-8c0a-1df22a427a87"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x \n",
    "except Exception: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2UtEHybluza"
   },
   "outputs": [],
   "source": [
    "# Orthographies to be tested\n",
    "LANGUAGES = ['eno', 'ent',\n",
    "             'ar', 'br', 'de', 'en', 'es',\n",
    "             'fi', 'fr', 'fro', 'it', 'ko',\n",
    "             'nl', 'pt', 'ru', 'sh', 'tr']\n",
    "TASKS = ['write', 'read']\n",
    "\n",
    "# Artificial Neural Network (ANN) hyperparameters\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "BATCH_SIZE = 32  \n",
    "NB_EPOCHS = 100  \n",
    "NB_LSTM_UNITS = 512  \n",
    "NB_MAX_SYMBOLS = 70\n",
    "\n",
    "NB_TRAINING_SAMPLES = 12500\n",
    "NB_TEST_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "kLiXtbnFvA7U",
    "outputId": "17e110ca-d1a8-4a2b-bd00-f9012215c995",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if GPU support\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsM1dJyXluzN"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pprint\n",
    "import difflib\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install watermark\n",
    "import watermark\n",
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distribution(error_positions, lang):\n",
    "  nb_positions = error_positions.shape[0]\n",
    "  print('nb_positions:', nb_positions)\n",
    "  labels = [x for x in range(0, nb_positions)]\n",
    "  print(labels)\n",
    "  df_error_distribitution = pd.DataFrame({'error_position': labels, 'nb_errors': error_positions})\n",
    "\n",
    "  ax = df_error_distribitution.plot.bar(x='error_position', y='nb_errors', rot=0, color='#607c8e')\n",
    "  plt.title('Location of the error')\n",
    "  plt.xlabel('Position')\n",
    "  plt.ylabel('Number of errors')\n",
    "  plt.grid(axis='y', alpha=0.75)\n",
    "  plt.show()\n",
    "  plt.savefig(lang+'_errors_position.png')\n",
    "\n",
    "def test_distribution():\n",
    "  nb=10\n",
    "  error_positions = np.zeros((nb,), dtype=int)\n",
    "  nb_errors=0\n",
    "  for i in range(nb):\n",
    "    error_positions[i]=i\n",
    "    nb_errors+=i\n",
    "  error_positions = error_positions/nb_errors\n",
    "  display_distribution(error_positions)\n",
    "#test_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4NHSVhBehnS"
   },
   "outputs": [],
   "source": [
    "def get_diff_symbols(wanted, predicted, task, debug=False):\n",
    "    \n",
    "    a = wanted\n",
    "    b = predicted\n",
    "    \n",
    "    error = False\n",
    "    error_position = 0\n",
    "\n",
    "    if debug:\n",
    "        print('{} => {}'.format(a,b))  \n",
    "    more = '+.'\n",
    "    less = '-.'\n",
    "    for i,s in enumerate(difflib.ndiff(a, b)):\n",
    "        if s[0]==' ': \n",
    "            continue\n",
    "        if s[0]=='-':\n",
    "          if debug:\n",
    "            print(u'- \\'{}\\' @ {}, '.format(s[-1],i))\n",
    "          if error == False:\n",
    "            error_position=i+1\n",
    "            error = True\n",
    "          less += s[-1] +'.'\n",
    "        elif s[0]=='+':\n",
    "          if debug:\n",
    "            print(u'+ \\'{}\\' @ {}, '.format(s[-1],i))\n",
    "          if error == False:\n",
    "            error_position=i+1\n",
    "            error = True  \n",
    "          more += s[-1]+'.'  \n",
    "    return task[0]+':'+less+more, error_position\n",
    "\n",
    "#get_diff_symbols('blinc√°remos', 'blincaremos', 'writing', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4LHmg-Gluzk"
   },
   "outputs": [],
   "source": [
    "''' One-hot encode a list of samples (e.g. sounds or letters) coded as list\n",
    "    of integer tokens into a one-hot encoded list of elements.\n",
    "    num_classes must not include value for 0. (eg. if 3 integers token 1, 2, 3)\n",
    "    one column will be automatically added by one_hot_encode for token value 0\n",
    "    This additional '0' class is needed to cope with keras Tokenizer library\n",
    "'''\n",
    "\n",
    "\n",
    "def one_hot_encode(integer_list_list, num_classes, verbose=False):\n",
    "    nb_i = len(integer_list_list)\n",
    "    nb_j = len(integer_list_list[0])\n",
    "    if verbose:\n",
    "        print('nb of samples:', nb_i)\n",
    "        print('sequence length:', nb_j)\n",
    "        print('number of tokens:', num_classes)\n",
    "    results = np.zeros((nb_i, nb_j, num_classes+1))  # +1 is for 0 (no-value)\n",
    "\n",
    "    for i, integer_list in enumerate(integer_list_list):\n",
    "        for j, integer in enumerate(integer_list):\n",
    "            for k in range(num_classes+1):\n",
    "                if k == integer:\n",
    "                    results[i, j, k] = 1\n",
    "                    break\n",
    "\n",
    "    return(results)\n",
    "\n",
    "#one_hot_encode([[1, 2, 3], [2, 2, 2]], 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPHJLbfPnD2m"
   },
   "outputs": [],
   "source": [
    "''' Retrieve the dataset for a given language.\n",
    "'''\n",
    "\n",
    "\n",
    "def get_data(lang):\n",
    "\n",
    "    filename = lang + '_wikt_samples.csv'\n",
    "    df = pd.read_csv(filename, keep_default_na=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QQxVvw4luzo"
   },
   "outputs": [],
   "source": [
    "''' Given a DataFrame shape, check if there are enough\n",
    "    samples to perform a training of nb_training_samples\n",
    "    and a test of nb_test_samples.\n",
    "    If not, decrease nb_training_samples accordingly.\n",
    "'''\n",
    "\n",
    "\n",
    "def get_nb_samples(df, nb_training_samples, nb_test_samples):\n",
    "    nb_samples = df.shape[0]\n",
    "    print('nb_samples:', nb_samples)\n",
    "    if nb_samples < (nb_training_samples + nb_test_samples):\n",
    "        nb_training_samples = df.shape[0] - nb_test_samples\n",
    "        print('check_dataset_size(): wanted nb_samples=%d : \\\n",
    "        not enough samples => nb_training_samples:%d !!!'\n",
    "              % (nb_samples, nb_training_samples))\n",
    "\n",
    "    return nb_training_samples, nb_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oGrUD1cKkw2"
   },
   "outputs": [],
   "source": [
    "''' Remove an index from a dictionary.\n",
    "'''\n",
    "\n",
    "\n",
    "def copy_and_remove_symbols(symbol_dict, symbols):\n",
    "    symbol_dict2 = symbol_dict.copy()\n",
    "    for key in symbols:\n",
    "        if key in symbol_dict2:\n",
    "            del symbol_dict2[key]\n",
    "    return symbol_dict2\n",
    "\n",
    "#copy_and_remove_symbols({ 'a':2, 'b':3, 'c':4, 'd':5}, ['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orID9AGwnriB"
   },
   "outputs": [],
   "source": [
    "def get_model_parameters(hist_params, df, input_name, target_name):\n",
    "\n",
    "    df['Input'] = df[input_name]\n",
    "    df['Target_0'] = df[target_name]\n",
    "    df['Target'] = df[target_name] + '>'\n",
    "    df['Target_input'] = '<' + df[target_name]\n",
    "\n",
    "    nb_samples = df.shape[0]\n",
    "\n",
    "    max_len_input = len(max(df['Input'].values, key=len))\n",
    "    max_len_target = len(max(df['Target'].values, key=len))\n",
    "    if max_len_target >= max_len_input:\n",
    "        print('max_len_target >= max_len_input ... but do nothing')\n",
    "\n",
    "    # tokenize the inputs\n",
    "    tokenizer_inputs = Tokenizer(num_words=NB_MAX_SYMBOLS, filters='', char_level=True)\n",
    "    tokenizer_inputs.fit_on_texts(df['Input'].values)\n",
    "    input_symbol2idx = tokenizer_inputs.word_index\n",
    "    nb_input_symbols = len(input_symbol2idx.keys())\n",
    "\n",
    "    # tokenize the outputs\n",
    "    tokenizer_outputs = Tokenizer(num_words=NB_MAX_SYMBOLS, filters='', char_level=True)\n",
    "    tokenizer_outputs.fit_on_texts(df['Target'].values + ['<'])\n",
    "    output_symbol2idx = tokenizer_outputs.word_index\n",
    "    print('output_symbol2idx:', output_symbol2idx.items())\n",
    "    nb_output_symbols = len(output_symbol2idx.keys())\n",
    "\n",
    "    # save main variables in the history to consult them after the test\n",
    "    hist_params['nb_samples'] = nb_samples\n",
    "    hist_params['max_len_input'] = max_len_input\n",
    "    hist_params['max_len_target'] = max_len_target\n",
    "\n",
    "    hist_params['tokenizer_inputs'] = tokenizer_inputs\n",
    "    hist_params['input_symbol2idx'] = input_symbol2idx\n",
    "    hist_params['nb_input_symbols'] = nb_input_symbols\n",
    "\n",
    "    hist_params['tokenizer_outputs'] = tokenizer_outputs\n",
    "    hist_params['output_symbol2idx'] = output_symbol2idx\n",
    "    hist_params['nb_output_symbols'] = nb_output_symbols\n",
    "\n",
    "    hist_params['x_symbols'] = ', '.join(sorted(input_symbol2idx.keys()))\n",
    "    output_symbols = copy_and_remove_symbols(output_symbol2idx, [\"<\", \">\", \"[\"])\n",
    "    hist_params['y_symbols'] = ', '.join(sorted(output_symbols.keys()))\n",
    "    hist_params['x_nb_symbols'] = len(input_symbol2idx)\n",
    "    hist_params['y_nb_symbols'] = len(output_symbols)\n",
    "\n",
    "    print('input symbols:', hist_params['x_symbols'])\n",
    "    print('input symbols nb:', hist_params['x_nb_symbols'])\n",
    "    print('output symbols:', hist_params['y_symbols'])\n",
    "    print('output symbols nb:', hist_params['y_nb_symbols'])\n",
    "\n",
    "    return df, hist_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zck_T6p5lu0B"
   },
   "outputs": [],
   "source": [
    "''' Split a dataframe into a training and a testing dataframes.\n",
    "'''\n",
    "\n",
    "\n",
    "def get_train_and_test_data(df, nb_training_samples, nb_test_samples):\n",
    "\n",
    "    df_train_test = df.sample(nb_training_samples + nb_test_samples)\n",
    "    df_train = df_train_test.head(nb_training_samples)\n",
    "    df_test = df_train_test.tail(nb_test_samples)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RsyHLmHlu0E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Prepare the ANN's input and output data by tokenizing, \n",
    "    padding and one-hot encoding them\n",
    "'''\n",
    "\n",
    "\n",
    "def prepare_input_and_target_format(hist_params, df_train):\n",
    "\n",
    "    inputs = df_train['Input'].values\n",
    "    targets = df_train['Target'].values\n",
    "    targets_inputs = df_train['Target_input'].values\n",
    "\n",
    "    tokenizer_inputs = hist_params['tokenizer_inputs']\n",
    "    tokenizer_outputs = hist_params['tokenizer_outputs']\n",
    "    max_len_input = hist_params['max_len_input']\n",
    "    max_len_target = hist_params['max_len_target']\n",
    "    nb_input_symbols = hist_params['nb_input_symbols']\n",
    "    nb_output_symbols = hist_params['nb_output_symbols']\n",
    "\n",
    "    # tokenize the input and output words (i.e. transform them into list of characters)\n",
    "    inputs_tk = tokenizer_inputs.texts_to_sequences(inputs)\n",
    "    targets_tk = tokenizer_outputs.texts_to_sequences(targets)\n",
    "    targets_inputs_tk = tokenizer_outputs.texts_to_sequences(targets_inputs)\n",
    "\n",
    "    # pad the input and output words (i.e. give each entry the same maxlen)\n",
    "    inputs_tk_pd = pad_sequences(inputs_tk, maxlen=max_len_input)\n",
    "    targets_inputs_tk_pd = pad_sequences(targets_inputs_tk, maxlen=max_len_target,)\n",
    "    targets_tk_pd = pad_sequences(targets_tk, maxlen=max_len_target, padding='post')\n",
    "\n",
    "    # one-hot encode the input and output words\n",
    "    inputs_tk_pd_hot = one_hot_encode(inputs_tk_pd, num_classes=nb_input_symbols)\n",
    "    targets_inputs_tk_pd_hot = one_hot_encode(targets_inputs_tk_pd, num_classes=nb_output_symbols)\n",
    "    targets_tk_pd_hot = one_hot_encode(targets_tk_pd, num_classes=nb_output_symbols)\n",
    "\n",
    "    return inputs_tk_pd_hot, targets_inputs_tk_pd_hot, targets_tk_pd_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AzKEcSrlu0L"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKyd3K3Olu0I"
   },
   "outputs": [],
   "source": [
    "def create_model(hist_params, verbose=True):\n",
    "\n",
    "    max_len_input = hist_params['max_len_input']\n",
    "    nb_input_symbols = hist_params['nb_input_symbols']\n",
    "    max_len_target = hist_params['max_len_target']\n",
    "    nb_output_symbols = hist_params['nb_output_symbols']\n",
    "\n",
    "    encoder_inputs_sounds = Input(shape=(max_len_input, nb_input_symbols+1,),\n",
    "                                  name=\"encoder_inputs_sounds\")\n",
    "\n",
    "    encoder = LSTM(\n",
    "        NB_LSTM_UNITS,\n",
    "        return_state=True,\n",
    "        name='encoder_lstm',\n",
    "    )\n",
    "\n",
    "    encoder_outputs, h, c = encoder(encoder_inputs_sounds)\n",
    "\n",
    "    # keep the states to pass them later into the decoder\n",
    "    encoder_states_1_2 = [h, c]\n",
    "\n",
    "    decoder_inputs_placeholder = Input(shape=(max_len_target, nb_output_symbols+1),\n",
    "                                       name=\"output_letters_help\")\n",
    "\n",
    "    decoder_inputs_x = decoder_inputs_placeholder\n",
    "\n",
    "    decoder_lstm = LSTM(\n",
    "        NB_LSTM_UNITS,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name='decoder_lstm',\n",
    "    )\n",
    "    decoder_outputs, _, _ = decoder_lstm(\n",
    "        decoder_inputs_x,\n",
    "        initial_state=encoder_states_1_2,\n",
    "    )\n",
    "\n",
    "    decoder_dense = Dense(nb_output_symbols+1, activation='softmax',\n",
    "                          name='decoder_sf_output_letters')\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs_sounds, decoder_inputs_placeholder], decoder_outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    hist_params['trainable_params'] = model.count_params()\n",
    "    print('trainable_params:', model.count_params())\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model_modules = {}\n",
    "    model_modules['encoder_inputs_sounds'] = encoder_inputs_sounds\n",
    "    model_modules['encoder_states_1_2'] = encoder_states_1_2\n",
    "    model_modules['decoder_lstm'] = decoder_lstm\n",
    "    model_modules['decoder_dense'] = decoder_dense\n",
    "\n",
    "    return model, model_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dBvTg8Olu0P"
   },
   "outputs": [],
   "source": [
    "def train_model(hist_params, model, encoder_inputs, decoder_inputs, decoder_targets, verbose=True):\n",
    "\n",
    "    callbacks_list = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=0.001,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    training_t0 = datetime.datetime.now()\n",
    "\n",
    "    r = model.fit(\n",
    "        [encoder_inputs, decoder_inputs], decoder_targets,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NB_EPOCHS,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_split=0.2,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    training_t1 = datetime.datetime.now()\n",
    "    training_duration = training_t1 - training_t0\n",
    "    print('training_duration:', training_duration)\n",
    "    hist_params['training_duration'] = str(training_duration)\n",
    "\n",
    "    val_accuracy = r.history.get('val_accuracy')[-1]\n",
    "    val_accuracy = float(int(val_accuracy * 1000)/1000)\n",
    "    print('val_accuracy:%.2f' % val_accuracy)\n",
    "\n",
    "    hist_params['val_accuracy'] = val_accuracy\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQolznVvjDi2"
   },
   "source": [
    "## Build the test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o95ukwfYlu0U"
   },
   "outputs": [],
   "source": [
    "def create_test_model(hist_params, model_modules, verbose=False):\n",
    "\n",
    "    encoder_inputs_sounds = model_modules['encoder_inputs_sounds']\n",
    "    encoder_states_1_2 = model_modules['encoder_states_1_2']\n",
    "    decoder_lstm = model_modules['decoder_lstm']\n",
    "    decoder_dense = model_modules['decoder_dense']\n",
    "\n",
    "    nb_output_symbols = hist_params['nb_output_symbols']\n",
    "\n",
    "    encoder_model = Model(encoder_inputs_sounds, encoder_states_1_2)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(NB_LSTM_UNITS,), name=\"lstm_cell_h\")\n",
    "    decoder_state_input_c = Input(shape=(NB_LSTM_UNITS,), name=\"lstm_cell_c\")\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_inputs_single = Input(shape=(1, nb_output_symbols+1,), name=\"reinput_letter\")\n",
    "\n",
    "    decoder_outputs, h, c = decoder_lstm(\n",
    "        decoder_inputs_single,\n",
    "        initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_states = [h, c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "      [decoder_inputs_single] + decoder_states_inputs,\n",
    "      [decoder_outputs] + decoder_states\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(encoder_model.summary())\n",
    "        print(decoder_model.summary())\n",
    "\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSfym65Nlu0W"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(hist_params, encoder_model, decoder_model, input_seq, verbose=False):\n",
    "\n",
    "    nb_output_symbols = hist_params['nb_output_symbols']\n",
    "    max_len_target = hist_params['max_len_target']\n",
    "    tokenizer_outputs = hist_params['tokenizer_outputs']\n",
    "    output_symbol2idx = hist_params['output_symbol2idx']\n",
    "\n",
    "    if verbose:\n",
    "        print('input_seq.shape :', input_seq.shape)\n",
    "\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    idx_start_tk = tokenizer_outputs.word_index['<']\n",
    "    target_seq = np.zeros((1, 1, nb_output_symbols+1))\n",
    "    target_seq[0, 0, idx_start_tk] = np.int64(1)\n",
    "\n",
    "    idx_end_tk = tokenizer_outputs.word_index['>']\n",
    "\n",
    "    # generate the step-by-step output prediction\n",
    "    output_sentence = []\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value\n",
    "        )\n",
    "\n",
    "        # Get next char\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if idx == idx_end_tk:\n",
    "            break\n",
    "\n",
    "        # reset the target_set (in particular, its symbols' row)\n",
    "        target_seq = np.zeros((1, 1, nb_output_symbols+1))\n",
    "\n",
    "        if idx in tokenizer_outputs.index_word.keys():\n",
    "            letter = tokenizer_outputs.index_word[idx]\n",
    "            output_sentence.append(letter)\n",
    "            target_seq[0, 0, idx] = 1\n",
    "        elif idx == 0:\n",
    "            target_seq[0, 0, idx] = 1\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ''.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLKO5zapGszk"
   },
   "outputs": [],
   "source": [
    "def decode_sequence_old(hist_params, encoder_model, decoder_model, input_seq, verbose=False):\n",
    "\n",
    "    nb_output_symbols = hist_params['nb_output_symbols']\n",
    "    max_len_target = hist_params['max_len_target']\n",
    "    tokenizer_outputs = hist_params['tokenizer_outputs']\n",
    "    output_symbol2idx = hist_params['output_symbol2idx']\n",
    "\n",
    "    if verbose:\n",
    "        print('input_seq.shape :', input_seq.shape)\n",
    "\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, nb_output_symbols+1))\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # NOTE: tokenizer lower-cases all words\n",
    "    test_letter_tk = tokenizer_outputs.texts_to_sequences('<')\n",
    "    test_letter_tk_pd = pad_sequences(test_letter_tk, maxlen=max_len_target)\n",
    "    test_letter_tk_pd_ht = one_hot_encode(test_letter_tk_pd, num_classes=nb_output_symbols)\n",
    "    target_seq[0, 0] = test_letter_tk_pd_ht[0][-1]\n",
    "\n",
    "    # be careful: eos type is np.int64\n",
    "    eos = tokenizer_outputs.texts_to_sequences('>')\n",
    "\n",
    "    # Create the translation\n",
    "    output_sentence = []\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value\n",
    "        )\n",
    "\n",
    "        # Get next word\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            letter = tokenizer_outputs.index_word[idx]\n",
    "            output_sentence.append(letter)\n",
    "\n",
    "        try:\n",
    "            test_letter_tk = tokenizer_outputs.texts_to_sequences(letter)\n",
    "            test_letter_tk_pd = pad_sequences(test_letter_tk, maxlen=max_len_target)\n",
    "            test_letter_tk_pd_ht = one_hot_encode(test_letter_tk_pd, num_classes=nb_output_symbols)\n",
    "            target_seq[0, 0] = test_letter_tk_pd_ht[0][-1]\n",
    "            if verbose:\n",
    "                print('target_seq[0, 0]:', target_seq[0, 0])\n",
    "        except:\n",
    "            # should not happen\n",
    "            print('!?')\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        # states_value = [h] # gru\n",
    "\n",
    "    return ''.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C96vM5Bvlu0W"
   },
   "outputs": [],
   "source": [
    "'''tokenize, pad, and one_hot_encode a word so that it can be input to ANN encoder'''\n",
    "\n",
    "\n",
    "def tpo(hist_params, sounds):\n",
    "    max_len_input = hist_params['max_len_input']\n",
    "    nb_input_symbols = hist_params['nb_input_symbols']\n",
    "    tokenizer_inputs = hist_params['tokenizer_inputs']\n",
    "    sounds_tk = tokenizer_inputs.texts_to_sequences(sounds)\n",
    "    sounds_tk_pd = pad_sequences(sounds_tk, maxlen=max_len_input)\n",
    "    sounds_tk_pd_ht = one_hot_encode(sounds_tk_pd, num_classes=nb_input_symbols)\n",
    "    return sounds_tk_pd_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QwccSNZlu0g"
   },
   "outputs": [],
   "source": [
    "def test(hist_params, encoder_model, decoder_model, df_test, verbose=False):\n",
    "\n",
    "    lang = hist_params['lang']\n",
    "    task = hist_params['task']\n",
    "\n",
    "    df_test['Prediction'] = ''\n",
    "    df_test['Prediction_diff'] = '='\n",
    "    max_len_target = hist_params['max_len_target']\n",
    "\n",
    "    # array indicating at which ouput position an error happened\n",
    "    test_err_index = np.zeros((max_len_target+1,), dtype=int)\n",
    "\n",
    "    test_t1 = datetime.datetime.now()\n",
    "\n",
    "    nb_predictions = 0\n",
    "    nb_good_predictions = 0\n",
    "    nb_bad_predictions = 0\n",
    "\n",
    "    for i, sample in df_test.iterrows():\n",
    "        # Do some test translations\n",
    "        x = sample['Input']\n",
    "\n",
    "        wanted_y = sample['Target_0']\n",
    "        if verbose:\n",
    "            print('x:', x, end='')\n",
    "            print(' wanted_y:', wanted_y, end='')\n",
    "        wiki_page = sample['Wiki_page']\n",
    "        predicted_y = decode_sequence(hist_params, encoder_model, decoder_model,\n",
    "                                      tpo(hist_params, [x]), verbose)\n",
    "        df_test.at[i, 'Prediction'] = predicted_y\n",
    "        if verbose:\n",
    "            print(' predicted_y:', predicted_y)\n",
    "\n",
    "        nb_predictions += 1\n",
    "        if predicted_y == wanted_y:\n",
    "            nb_good_predictions += 1\n",
    "        else:\n",
    "            nb_bad_predictions += 1\n",
    "            df_test.at[i, 'Prediction_diff'], error_position = get_diff_symbols(wanted_y, predicted_y, task)\n",
    "            test_err_index[error_position] += 1\n",
    "\n",
    "        if nb_predictions == 1:\n",
    "            print(\"[[%s]]: input:%s -> predicted:%s, wanted:%s, is_ok=%s\" %\n",
    "                  (wiki_page, x, predicted_y, wanted_y, predicted_y == wanted_y))\n",
    "\n",
    "    test_accuracy = round(float(nb_good_predictions*100/nb_predictions))/100\n",
    "\n",
    "    print('lang:%s, task:%s, nb_good_predictions=%d out of %d trials, test_accuracy=%.2f' %\n",
    "          (lang, task, nb_good_predictions, nb_predictions, test_accuracy))\n",
    "\n",
    "    test_t2 = datetime.datetime.now()\n",
    "    test_duration = test_t2 - test_t1\n",
    "    print(\"test_duration:\", test_duration)\n",
    "\n",
    "    # keep trace of the results\n",
    "    hist_params['nb_predictions'] = nb_predictions\n",
    "    hist_params['nb_good_predictions'] = nb_good_predictions\n",
    "    hist_params['test_err_index'] = test_err_index/nb_bad_predictions\n",
    "    hist_params['test_accuracy'] = test_accuracy\n",
    "    hist_params['test_duration'] = str(test_duration)\n",
    "\n",
    "    # keep trace of the top-three most common mistakes\n",
    "    # as well as one example of word for which the mistake has happened\n",
    "    rank = 1\n",
    "    for key, val in df_test.Prediction_diff[df_test.Prediction_diff != '='].value_counts().head(3).items():\n",
    "        print(rank, key, val)\n",
    "        hist_params['test_err_'+str(rank)+'_label'] = str(key)\n",
    "        hist_params['test_err_'+str(rank)+'_nb'] = str(val)\n",
    "        for key2, val2 in df_test[df_test.Prediction_diff == key].sample(1).iterrows():\n",
    "            hist_params['test_err_'+str(rank)+'_word'] = val2['Word']\n",
    "            hist_params['test_err_'+str(rank)+'_pronunciation'] = val2['Pronunciation']\n",
    "            hist_params['test_err_'+str(rank)+'_prediction'] = val2['Prediction']\n",
    "        rank += 1\n",
    "\n",
    "    print('test_err_index:', hist_params['test_err_index'])\n",
    "    display_distribution(hist_params['test_err_index'], hist_params['lang'])\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BEcpQ4xGQrD"
   },
   "outputs": [],
   "source": [
    "UNIT_TEST = False\n",
    "if UNIT_TEST:\n",
    "\n",
    "    hist_params = {\n",
    "        'lang': 'de'\n",
    "    }\n",
    "    input_name = 'Pronunciation'\n",
    "    target_name = 'Word'\n",
    "    task = 'write'\n",
    "    verbose = True\n",
    "\n",
    "    lang = hist_params['lang']\n",
    "    hist_params['task'] = task\n",
    "    hist_params['lstm'] = NB_LSTM_UNITS\n",
    "\n",
    "    print(\"1/ GET DATA (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    df = get_data(lang)\n",
    "    nb_test_samples = NB_TEST_SAMPLES    \n",
    "    nb_training_samples, nb_test_samples = get_nb_samples(df, NB_TRAINING_SAMPLES, nb_test_samples)\n",
    "\n",
    "    hist_params['word_len_mean'] = int(df['Word'].str.len().mean()*10)/10\n",
    "    hist_params['pronunciation_len_mean'] = int(df['Pronunciation'].str.len().mean()*10)/10\n",
    "\n",
    "    print(\"2/ PREPARE DATA (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    df, hist_params = get_model_parameters(hist_params, df, input_name=input_name, target_name=target_name)\n",
    "    df_train, df_test = get_train_and_test_data(df, nb_training_samples, nb_test_samples)\n",
    "    df_train.head(5)\n",
    "    encoder_inputs, decoder_inputs, decoder_targets = prepare_input_and_target_format(hist_params, df_train)\n",
    "    print('(nb_encoder_input.shape (nb_inputs, nb_letters, nb_symbols) :', encoder_inputs.shape)\n",
    "    print('(nb_decoder_input.shape (nb_outputs, nb_letters, nb_symbols) :', decoder_inputs.shape)\n",
    "    print('(nb_decoder_target.shape (nb_outputs, nb_letters, nb_symbols) :', decoder_targets.shape)\n",
    "\n",
    "    print(\"3/ BUILD MODEL (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    model, model_modules = create_model(hist_params, verbose)\n",
    "\n",
    "    print(\"4/ TRAIN MODEL (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    r = train_model(hist_params, model, encoder_inputs, decoder_inputs, decoder_targets, verbose=True)\n",
    "\n",
    "    # plot some data\n",
    "    plt.plot(r.history['loss'], label='loss')\n",
    "    plt.plot(r.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # accuracies\n",
    "    plt.plot(r.history['accuracy'], label='accuracy')\n",
    "    plt.plot(r.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    encoder_model, decoder_model = create_test_model(hist_params, model_modules, verbose=False)\n",
    "\n",
    "    print(\"5/ TEST MODEL (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    # test NB_TEST_SAMPLES samples\n",
    "    df_test_results = test(hist_params, encoder_model, decoder_model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdRaBPR6lu0Q"
   },
   "outputs": [],
   "source": [
    "def make_and_test_model(hist_params, input_name, target_name, task, verbose):\n",
    "    lang = hist_params['lang']\n",
    "    # save parameters for history\n",
    "    hist_params['task'] = task\n",
    "    hist_params['lstm'] = NB_LSTM_UNITS\n",
    "\n",
    "    print(\"1/ GET DATA (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    df = get_data(lang)  \n",
    "    nb_training_samples, nb_test_samples = get_nb_samples(df, NB_TRAINING_SAMPLES, NB_TEST_SAMPLES)\n",
    "\n",
    "    hist_params['word_len_mean'] = int(df['Word'].str.len().mean()*10)/10\n",
    "    hist_params['pronunciation_len_mean'] = int(df['Pronunciation'].str.len().mean()*10)/10\n",
    "\n",
    "    print(\"2/ PREPARE DATA (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    df, hist_params = get_model_parameters(hist_params, df, input_name=input_name, target_name=target_name)\n",
    "    df_train, df_test = get_train_and_test_data(df, nb_training_samples, nb_test_samples)\n",
    "    df_train.head(5)\n",
    "    encoder_inputs, decoder_inputs, decoder_targets = prepare_input_and_target_format(hist_params, df_train)\n",
    "    print('(nb_encoder_input.shape (nb_inputs, nb_letters, nb_symbols) :', encoder_inputs.shape)\n",
    "    print('(nb_decoder_input.shape (nb_outputs, nb_letters, nb_symbols) :', decoder_inputs.shape)\n",
    "    print('(nb_decoder_target.shape (nb_outputs, nb_letters, nb_symbols) :', decoder_targets.shape)\n",
    "\n",
    "    print(\"3/ BUILD MODEL (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    model, model_modules = create_model(hist_params, verbose)\n",
    "\n",
    "    print(\"4/ TRAIN MODEL (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    r = train_model(hist_params, model, encoder_inputs, decoder_inputs, decoder_targets, verbose)\n",
    "    # Save the model\n",
    "    model.save(task + '_model_' + hist_params['lang'] + '.h5')\n",
    "\n",
    "    # plot some data\n",
    "    plt.plot(r.history['loss'], label='loss')\n",
    "    plt.plot(r.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # accuracies\n",
    "    plt.plot(r.history['accuracy'], label='accuracy')\n",
    "    plt.plot(r.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    encoder_model, decoder_model = create_test_model(hist_params, model_modules, verbose)\n",
    "\n",
    "    print(\"5/ TEST MODEL (%s, %s, %d)\" % (lang, task, NB_LSTM_UNITS))\n",
    "    # test NB_TEST_SAMPLES samples\n",
    "    df_test = test(hist_params, encoder_model, decoder_model, df_test)\n",
    "    df_test = df_test.drop(columns=['Input', 'Target_0', 'Target', 'Target_input'])\n",
    "    return df_test\n",
    "\n",
    "    print('####################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJtRsXc6MhAl"
   },
   "outputs": [],
   "source": [
    "def test_orthography(lang, task, verbose=False):\n",
    "    date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    time = datetime.datetime.today().strftime('%H:%M:%S')\n",
    "\n",
    "    hist_params = {'date': date, 'time': time, 'lang': lang}\n",
    "\n",
    "    if task == 'write':\n",
    "        df_test = make_and_test_model(hist_params, input_name='Pronunciation',\n",
    "                                      target_name='Word', task=task,\n",
    "                                      verbose=False)\n",
    "    elif task == 'read':\n",
    "        df_test = make_and_test_model(hist_params, input_name='Word',\n",
    "                                      target_name='Pronunciation', task=task,\n",
    "                                      verbose=False)\n",
    "    else:\n",
    "        print('test_orthography: task=%s does not exist' % task)\n",
    "\n",
    "    dict_res = copy_and_remove_symbols(hist_params, ['input_symbol2idx',\n",
    "                                                     'output_symbol2idx',\n",
    "                                                     'tokenizer_inputs',\n",
    "                                                     'tokenizer_outputs'])\n",
    "    if verbose:\n",
    "        pprint.pprint(dict_res)\n",
    "    return dict_res, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fbKkW0VQLEoW",
    "outputId": "6417dc19-dba7-454c-8fd7-f6218e62b47d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "output_prefix = 'oteann_results_' + date\n",
    "results_file = output_prefix + '.csv'\n",
    "\n",
    "# open the file for being able to append the results of this test\n",
    "# otherwise create a new one\n",
    "try:\n",
    "    df_results = pd.read_csv(results_file)\n",
    "except:\n",
    "    df_results = pd.DataFrame()\n",
    "df_last_test = pd.DataFrame()\n",
    "\n",
    "nb_episodes = 1\n",
    "for episode in range(nb_episodes):\n",
    "    print('episode:', episode)\n",
    "\n",
    "    for lang in LANGUAGES:\n",
    "        for task in TASKS:\n",
    "            dict_res, df_last_test = test_orthography(lang, task, verbose=False)\n",
    "            # put the results as a new line in the CSV history file\n",
    "            df_res = pd.DataFrame(data = [dict_res.values()], columns = dict_res.keys())\n",
    "            df_results = pd.concat([df_results, df_res], axis=0, ignore_index=True, sort=False)\n",
    "            df_results.to_csv(results_file, index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmNeZDBY2it9"
   },
   "outputs": [],
   "source": [
    "def display_results(df):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    current_palette = sns.color_palette()\n",
    "    for task in TASKS:\n",
    "        df_o = df[df.task==task].sort_values(by='lang')\n",
    "        df_o = df_o[['task', 'lang', 'test_accuracy']]\n",
    "        df_o['test_accuracy']=df_o['test_accuracy']*100\n",
    "        df_o = df_o.groupby('lang', as_index=False).mean()\n",
    "        sns.palplot(current_palette)\n",
    "        ax = sns.barplot(x=\"lang\", y=\"test_accuracy\",\n",
    "                         data=df_o, palette=current_palette)\n",
    "        # add the accuracy number on the top of each bar\n",
    "        i=0\n",
    "        for index, row in df_o.iterrows():\n",
    "            ax.text(i, row.test_accuracy+1, str(round(row.test_accuracy)), color='black', ha=\"center\")\n",
    "            i+=1\n",
    "        plt.title(task.capitalize())\n",
    "        plt.ylim(0, 100)\n",
    "        plt.show()\n",
    "\n",
    "display_results(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsxdseMxX-R-"
   },
   "outputs": [],
   "source": [
    "def display_distribution(test_err_index):\n",
    "    nb_indexes = test_err_index.shape[0]\n",
    "    labels = [x for x in range(0, nb_indexes)]\n",
    "    df_error_distribitution = pd.DataFrame({'error_position': labels,\n",
    "                                            'nb_errors': test_err_index})\n",
    "\n",
    "    ax = df_error_distribitution.plot.bar(x='error_position', y='nb_errors',\n",
    "                                          rot=0, color='#607c8e')\n",
    "    plt.title('Index of the badly predicted output character')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Number of errors')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()\n",
    "    plt.savefig(\"figx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_history(df):\n",
    "    for i, row in df.iterrows():\n",
    "        print('test:%d, LANG=%s' % (i, row.lang))\n",
    "        for column in df.columns:\n",
    "            print('%s:%s' % (column, str(row[column])))\n",
    "            if column == 'test_err_index':\n",
    "                display_distribution(row[column])\n",
    "        print('=================================================')\n",
    "display_history(df_results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "write_read_ANNs_v5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
